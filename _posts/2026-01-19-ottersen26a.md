---
title: Incorporating the Cycle Inductive Bias in Masked Autoencoders
openreview: qp5lal8QhP
abstract: Many time series exhibit cyclic structure   for example, in physiological
  signals such as ECG or EEG   yet most representation learning methods treat them
  as generic sequences. We propose a masked autoencoder (MAE) framework that explicitly
  leverages cycles as an inductive bias for more efficient and effective time-series
  modelling. Our method decomposes sequences into cycles and trains the model to reconstruct
  masked segments at both the cycle and sequence level. This cycle-based decomposition
  shortens the effective sequence length processed by the encoder by up to a factor
  of ten in our experiments, yielding substantial computational savings without loss
  in reconstruction quality. At the same time, the approach exposes the encoder to
  a greater diversity of temporal patterns, as each cycle forms an additional training
  instance, which enhances the ability to capture subtle intra-cycle variations. Empirically,
  our framework outperforms three competitive baselines across four cyclic datasets,
  while also reducing training time on larger datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ottersen26a
month: 0
tex_title: Incorporating the Cycle Inductive Bias in Masked Autoencoders
firstpage: 319
lastpage: 327
page: 319-327
order: 319
cycles: false
bibtex_author: Ottersen, Stuart Gallina and Bach, Kerstin
author:
- given: Stuart Gallina
  family: Ottersen
- given: Kerstin
  family: Bach
date: 2026-01-19
address:
container-title: Proceedings of the 7th Northern Lights Deep Learning Conference (NLDL)
volume: '307'
genre: inproceedings
issued:
  date-parts:
  - 2026
  - 1
  - 19
pdf: https://raw.githubusercontent.com/mlresearch/v307/main/assets/ottersen26a/ottersen26a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
